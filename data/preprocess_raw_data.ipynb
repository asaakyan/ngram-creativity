{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from unidecode import unidecode\n",
    "from Levenshtein import ratio as levenshtein_ratio\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_ratings_across_batches_df = pd.read_csv(\"raw/main_study/ratings_main.csv\")\n",
    "frontier_ratings_across_batches_df = pd.read_csv(\"raw/frontier_study/ratings_frontier.csv\")\n",
    "all_ratings_across_batches_df = pd.concat([base_ratings_across_batches_df, \n",
    "                                           frontier_ratings_across_batches_df], axis=0)\n",
    "\n",
    "base_highlights_across_batches_df = pd.read_csv(\"raw/main_study/highlights_main.csv\")\n",
    "frontier_highlights_across_batches_df = pd.read_csv(\"raw/frontier_study/highlights_frontier.csv\")\n",
    "all_highlights_across_batches_df = pd.concat([base_highlights_across_batches_df, \n",
    "                                              frontier_highlights_across_batches_df], axis=0)\n",
    "\n",
    "base_expr_data_df = pd.read_csv(\"raw/main_study/expr_data_main.csv\")\n",
    "frontier_expr_data_df = pd.read_csv(\"raw/frontier_study/expr_data_frontier.csv\")\n",
    "all_expr_data_df = pd.concat([base_expr_data_df, frontier_expr_data_df], axis=0)\n",
    "\n",
    "base_hlt_scores_df = pd.read_csv(\"raw/main_study/hlt_scores_main.csv\")\n",
    "base_hlt_scores_df['highlight_annot'] = 1\n",
    "base_hlt_scores_df['meaningful'] = 1\n",
    "base_hlt_scores_df['pragmatic'] = 1\n",
    "\n",
    "frontier_hlt_scores_df = pd.read_csv(\"raw/frontier_study/hlt_scores_frontier.csv\")\n",
    "frontier_hlt_scores_df['highlight_annot'] = 1\n",
    "frontier_hlt_scores_df['meaningful'] = 1\n",
    "frontier_hlt_scores_df['pragmatic'] = 1\n",
    "\n",
    "all_hlt_scores_df = pd.concat([base_hlt_scores_df, frontier_hlt_scores_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545 527 517\n",
      "0.03302752293577982 0.05137614678899083\n"
     ]
    }
   ],
   "source": [
    "total_nov_annots = base_ratings_across_batches_df[base_ratings_across_batches_df['novel'] == 1]['novel'].sum()\n",
    "nov_non_sense_annots = base_ratings_across_batches_df[base_ratings_across_batches_df['novel'] == 1]['meaningful'].sum()\n",
    "nov_non_prag_anntos = base_ratings_across_batches_df[base_ratings_across_batches_df['novel'] == 1]['pragmatic'].sum()\n",
    "print(total_nov_annots, nov_non_sense_annots, nov_non_prag_anntos)\n",
    "print(1-nov_non_sense_annots/total_nov_annots, 1-nov_non_prag_anntos/total_nov_annots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of ratings:  7542\n",
      "# unique expressions:  (7542, 12)\n",
      "# unique non-pragmatic:  (667, 12)\n",
      "# unique novel:  (545, 12)\n",
      "# unique non-sensical:  (252, 12)\n",
      "# of highlights:  226\n"
     ]
    }
   ],
   "source": [
    "print(\"# of ratings: \", base_ratings_across_batches_df.shape[0])\n",
    "print(\"# unique expressions: \", base_ratings_across_batches_df.shape)\n",
    "print(\"# unique non-pragmatic: \", base_ratings_across_batches_df[base_ratings_across_batches_df['pragmatic'] == 0].shape)\n",
    "print(\"# unique novel: \", base_ratings_across_batches_df[base_ratings_across_batches_df['novel'] == 1].shape)\n",
    "print(\"# unique non-sensical: \", base_ratings_across_batches_df[base_ratings_across_batches_df['meaningful'] == 0].shape)\n",
    "print(\"# of highlights: \", base_highlights_across_batches_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of ratings:  1076\n",
      "# unique expressions:  (1076, 12)\n",
      "# unique non-pragmatic:  (55, 12)\n",
      "# unique novel:  (44, 12)\n",
      "# unique non-sensical:  (22, 12)\n",
      "# of highlights:  15\n"
     ]
    }
   ],
   "source": [
    "print(\"# of ratings: \", frontier_ratings_across_batches_df.shape[0])\n",
    "print(\"# unique expressions: \", frontier_ratings_across_batches_df.shape)\n",
    "print(\"# unique non-pragmatic: \", frontier_ratings_across_batches_df[frontier_ratings_across_batches_df['pragmatic'] == 0].shape)\n",
    "print(\"# unique novel: \", frontier_ratings_across_batches_df[frontier_ratings_across_batches_df['novel'] == 1].shape)\n",
    "print(\"# unique non-sensical: \", frontier_ratings_across_batches_df[frontier_ratings_across_batches_df['meaningful'] == 0].shape)\n",
    "print(\"# of highlights: \", frontier_highlights_across_batches_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reg data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ratings_to_nov_scores(ratings_across_batches_df, expr_data_df):\n",
    "    \"\"\"\n",
    "    Merge expression data with ratings data for regression analysis.\n",
    "    \"\"\"\n",
    "    regr_df = ratings_across_batches_df.merge(expr_data_df,\n",
    "                                            on=['gen_passage_id', 'expression', \n",
    "                                            'batch', 'part'],\n",
    "                                            how='left'\n",
    "                                            ,suffixes=('', '_y'))\n",
    "    print(regr_df.shape)\n",
    "    assert regr_df['annotator_id'].isna().sum() == 0\n",
    "    assert regr_df['ppl'].isna().sum() == 0\n",
    "\n",
    "    cols_to_keep = ['batch', 'gen_passage_id', 'seed_passage_id', 'expression', \n",
    "                    'novel', 'meaningful', 'pragmatic', \n",
    "                    'annotator_id', 'gen_source',\n",
    "                    \"ppl\", \"median_smallest\", \"percent_smallest\", \n",
    "                    \"median_dclm_smallest\", \"percent_dclm_smallest\"]\n",
    "    regr_df = regr_df[cols_to_keep]\n",
    "    return regr_df\n",
    "\n",
    "def merge_nov_scores_to_ratings(ratings_across_batches_df, expr_data_df):\n",
    "    \"\"\"\n",
    "    Merge expression data with ratings data for regression analysis.\n",
    "    \"\"\"\n",
    "    regr_df = expr_data_df.merge(ratings_across_batches_df,\n",
    "                                            on=['gen_passage_id', 'expression', \n",
    "                                            'batch', 'part'],\n",
    "                                            how='left'\n",
    "                                            ,suffixes=('', '_y'))\n",
    "    print(regr_df.shape)\n",
    "    assert regr_df['ppl'].isna().sum() == 0\n",
    "\n",
    "    cols_to_keep = ['batch', 'gen_passage_id', 'seed_passage_id', 'expression', \n",
    "                    'novel', 'meaningful', 'pragmatic', \n",
    "                    'annotator_id', 'gen_source',\n",
    "                    \"ppl\", \"median_smallest\", \"percent_smallest\", \n",
    "                    \"median_dclm_smallest\", \"percent_dclm_smallest\"]\n",
    "    regr_df = regr_df[cols_to_keep]\n",
    "    return regr_df\n",
    "\n",
    "def compare_expr_to_hlts(expr: str, hlts: List[str], thresh = 0.9, debug=False):\n",
    "\n",
    "    if len(hlts) == 0:\n",
    "        return 0\n",
    "\n",
    "    if debug: print(f\"Expression: {expr}\")\n",
    "    for hl in hlts:\n",
    "        expr, hl = unidecode(expr), unidecode(hl)\n",
    "        if expr in hl or hl in expr:\n",
    "            if debug: print(f\"Expression '{expr}' is a subset/superset of the highlight '{hl}'\")\n",
    "            return 1\n",
    "        elif levenshtein_ratio(expr, hl) >= thresh:\n",
    "            print(f\"Expression '{expr}' is very similar to the highlight '{hl}'\")\n",
    "            return 1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique ids: 110\n",
      "Unique gen_sources: 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gen_passage_id</th>\n",
       "      <th>WQRM_score</th>\n",
       "      <th>ai_likelihood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18037_olmo1</td>\n",
       "      <td>6.650258</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gen_passage_id  WQRM_score  ai_likelihood\n",
       "0    18037_olmo1    6.650258            1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_annots = {}\n",
    "id_to_annots.update(\n",
    "    all_ratings_across_batches_df.groupby('gen_passage_id')['annotator_id'].unique().to_dict()\n",
    ")\n",
    "id_to_gen_source = {}\n",
    "id_to_gen_source.update(\n",
    "    all_ratings_across_batches_df.groupby('gen_passage_id')['gen_source'].first().to_dict()\n",
    ")\n",
    "print(f\"Unique ids: {len(id_to_annots)}\")\n",
    "print(f\"Unique gen_sources: {len(set(id_to_gen_source.values()))}\")\n",
    "\n",
    "wqrm_df = pd.read_csv(\"raw/main_study/wqrm_ail_scores.csv\")\n",
    "wqrm_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### raw annotations (pragmatics, meaningful data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8618, 33)\n",
      "(8618, 16)\n"
     ]
    }
   ],
   "source": [
    "base_front_regr_df = merge_ratings_to_nov_scores(all_ratings_across_batches_df,\n",
    "                                            all_expr_data_df)\n",
    "base_front_regr_df = base_front_regr_df.merge(wqrm_df,\n",
    "                                        on=['gen_passage_id'],\n",
    "                                        how='left')\n",
    "assert base_front_regr_df['WQRM_score'].isna().sum() == base_front_regr_df[base_front_regr_df['batch'] == 11].shape[0]  # batch 11 passages are not in the wqrm file\n",
    "assert base_front_regr_df['ai_likelihood'].isna().sum() == base_front_regr_df[base_front_regr_df['batch'] == 11].shape[0]  # batch 11 passages are not in the wqrm file\n",
    "\n",
    "cols_to_keep = ['batch', 'gen_passage_id', 'seed_passage_id',  \n",
    "                'novel', 'meaningful', 'pragmatic', \n",
    "                'annotator_id', 'gen_source',\n",
    "                'ppl', 'WQRM_score', 'ai_likelihood']\n",
    "print(base_front_regr_df.shape)\n",
    "base_front_regr_df[cols_to_keep].to_csv(\"for_linear_models/prehlt_only.csv\", index=False)\n",
    "base_front_regr_df[cols_to_keep + ['expression']].to_csv(\"for_linear_models/with_exprs/prehlt_only_w_exprs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### incorporate highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11758, 33)\n",
      "(11758, 14)\n",
      "Number of expressions containing annotation:  8618\n",
      "# expressions missing annotation:  3140\n",
      "Expected number of expression in the filled DF:  18038\n",
      "Expanded rows shape:  (9725, 14)\n",
      "Filled DF shape:  (18343, 14)\n",
      "Expression 'that she'd have to wait for the snowflakes to come at their own pace' is very similar to the highlight 'she'd have to wait for the snowflakes to come at their own pace.'\n",
      "Expression 'that she'd hooked up with a sort of invalid' is very similar to the highlight 'she'd hooked up with a sort of invalid.'\n",
      "Expression 'an aluminum container as big around as a baby's head' is very similar to the highlight ' an aluminum container as big around as a baby's hea'\n",
      "Expression 'and she found fairly steady work doing sadomasochistic accessories for a private club in the Village' is very similar to the highlight 'fairly steady work doing sadomasochistic accessories for a private club in the Village.'\n",
      "Expression '\" for his love of the game and the smoky haze of the poolrooms where he'd spent his formative years' is very similar to the highlight ' \"Billiards,\" for his love of the game and the smoky haze of the poolrooms where he'd spent his formative year'\n",
      "Number of expressions highlighted:  241\n",
      "Number of non-rated expressions that were part of highlights:  310\n",
      "Scored highlights shape:  (241, 15)\n",
      "Dims before adding highlights:  (18082, 14)\n",
      "Dims after adding highlights:  (18323, 15)\n",
      "Final filled DF shape:  (18323, 17)\n"
     ]
    }
   ],
   "source": [
    "regr_df = merge_nov_scores_to_ratings(all_ratings_across_batches_df, all_expr_data_df)\n",
    "print(regr_df.shape)\n",
    "# regr_df.head()\n",
    "\n",
    "na_mask = regr_df['annotator_id'].isna()\n",
    "print(\"Number of expressions containing annotation: \", regr_df[~na_mask].shape[0])\n",
    "missing_annotator_df = regr_df[na_mask].copy()\n",
    "print(\"# expressions missing annotation: \", missing_annotator_df.shape[0])\n",
    "print(\"Expected number of expression in the filled DF: \", missing_annotator_df.shape[0]*3 \n",
    "      + regr_df[~na_mask].shape[0])\n",
    "\n",
    "# fill in the missing annotations in the following way:\n",
    "# for each expression that was not annotated as novel\n",
    "# we assume it was annotated as non-novel (non-creative) by all annotators \n",
    "# (since if it were creative, they would have highlighted it)\n",
    "# safe to assume non-novel but not non-pragmatic or non-sensical, so we leave those as NaN\n",
    "expanded_rows = []\n",
    "for _, row in missing_annotator_df.iterrows():\n",
    "    annots = id_to_annots[row['gen_passage_id']]\n",
    "    assert len(annots) != 0\n",
    "    for a in annots:\n",
    "        r = row.copy()\n",
    "        r['annotator_id'] = a\n",
    "        r['gen_source'] = id_to_gen_source[row['gen_passage_id']]\n",
    "        r['novel'] = 0\n",
    "        expanded_rows.append(r)\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "# confirm each expression occurs exactly 3 times in the dataframe\n",
    "assert expanded_df[expanded_df['batch'] < 11].groupby('gen_passage_id')['expression'].value_counts().value_counts().index == 3\n",
    "# 4 annotators in batch 11\n",
    "assert expanded_df[expanded_df['batch'] == 11].groupby('gen_passage_id')['expression'].value_counts().value_counts().index == 4\n",
    "print(\"Expanded rows shape: \", expanded_df.shape)\n",
    "\n",
    "# concatenate to the regression df\n",
    "filled_regr_df = pd.concat([regr_df[~na_mask], expanded_df], ignore_index=True)\n",
    "assert filled_regr_df['novel'].isna().sum() == 0    \n",
    "filled_regr_df = filled_regr_df.sort_values(by=['batch', 'gen_passage_id', 'expression']).reset_index(drop=True)\n",
    "print(\"Filled DF shape: \", filled_regr_df.shape)\n",
    "# filled_regr_df.head(7)\n",
    "\n",
    "# now, we want to drop any expressions that are sub/supersets or very similar to any highlighted expressions\n",
    "filled_regr_df['novel_highlight'] = filled_regr_df.apply(lambda row:\n",
    "        compare_expr_to_hlts(\n",
    "            row['expression'], \n",
    "            # novel higlights from that annotator for that passage\n",
    "            all_highlights_across_batches_df[\n",
    "                (all_highlights_across_batches_df['annotator_id'] == row['annotator_id'])\n",
    "                & (all_highlights_across_batches_df['gen_passage_id'] == row['gen_passage_id'])\n",
    "            ]['novel_expr'].tolist(),\n",
    "            thresh=0.9, debug=False\n",
    "        ), axis=1\n",
    "    )\n",
    "print(\"Number of expressions highlighted: \", all_highlights_across_batches_df.shape[0])\n",
    "print(\"Number of non-rated expressions that were part of highlights: \", filled_regr_df['novel_highlight'].sum())\n",
    "# filled_regr_df[filled_regr_df['novel_highlight'] == 1].head(7)\n",
    "\n",
    "# now, we will drop all expressions that are parts of novel highlights \n",
    "# instead, we will add the novel highlights with their novelty scores directly\n",
    "# note that we keep any pre-highlighted expression annotations, only drop the assumed annotations\n",
    "# this risks overcounting some novel expressions, \n",
    "# but strictly speaking there is no requirement that subpart of a novel expression is novel\n",
    "filled_regr_NoNovHlts_df = filled_regr_df[(filled_regr_df['novel_highlight'] == 0)\n",
    "                                          |\n",
    "                                          ( (filled_regr_df['novel_highlight'] == 1)\n",
    "                                           # this is to ensure we do not drop pragmatic annotations \n",
    "                                           # even for subparts of creative expressions\n",
    "                                          & (~filled_regr_df['pragmatic'].isna()) \n",
    "                                          )\n",
    "                                          ].copy()\n",
    "# drop the novel_highlight column\n",
    "filled_regr_NoNovHlts_df.drop(columns=['novel_highlight'], inplace=True)\n",
    "\n",
    "print(\"Scored highlights shape: \", all_hlt_scores_df.shape)\n",
    "print(\"Dims before adding highlights: \", filled_regr_NoNovHlts_df.shape)\n",
    "filled_regr_replNovHlts_df = pd.concat([filled_regr_NoNovHlts_df, all_hlt_scores_df], ignore_index=True)\n",
    "assert filled_regr_replNovHlts_df['novel'].isna().sum() == 0\n",
    "assert (filled_regr_replNovHlts_df[~filled_regr_replNovHlts_df['pragmatic'].isna()].shape[0] \n",
    "        == all_ratings_across_batches_df.shape[0] + all_highlights_across_batches_df.shape[0])\n",
    "print(\"Dims after adding highlights: \", filled_regr_replNovHlts_df.shape)\n",
    "\n",
    "assert (regr_df[regr_df['annotator_id'].notna()].shape[0] # number of annotated expressions\n",
    " + regr_df[regr_df['batch'] < 11]['annotator_id'].isna().sum() * 3 # 3x missing annotations bc assume 3 annotators rated as non-novel\n",
    " + regr_df[regr_df['batch'] == 11]['annotator_id'].isna().sum() * 4 # 4x missing annotations for batch 11\n",
    " - filled_regr_df[filled_regr_df['pragmatic'].isna()]['novel_highlight'].sum() # minus the number of expressions that were part of highlights\n",
    "+ all_hlt_scores_df.shape[0] # plus the number of novel highlights with scores\n",
    " ) == filled_regr_replNovHlts_df.shape[0]\n",
    "# print(filled_regr_replNovHlts_df.drop_duplicates(['gen_passage_id', 'expression']).shape)\n",
    "\n",
    "filled_regr_replNovHlts_df = filled_regr_replNovHlts_df.merge(wqrm_df,\n",
    "                                        on=['gen_passage_id'],\n",
    "                                        how='left')\n",
    "assert (filled_regr_replNovHlts_df['WQRM_score'].isna().sum() == filled_regr_replNovHlts_df[filled_regr_replNovHlts_df['batch'] == 11].shape[0])  # batch 11 passages are not in the wqrm file\n",
    "assert (filled_regr_replNovHlts_df['ai_likelihood'].isna().sum() == filled_regr_replNovHlts_df[filled_regr_replNovHlts_df['batch'] == 11].shape[0])  # batch 11 passages are not in the wqrm file\n",
    "\n",
    "cols_to_keep = ['batch', 'gen_passage_id', 'seed_passage_id', \n",
    "                'novel', 'meaningful', 'pragmatic', \n",
    "                'annotator_id', 'gen_source',\n",
    "                'ppl', 'WQRM_score', 'ai_likelihood']\n",
    "print(\"Final filled DF shape: \", filled_regr_replNovHlts_df.shape)\n",
    "filled_regr_replNovHlts_df.head()\n",
    "filled_regr_replNovHlts_df[cols_to_keep].to_csv(\"for_linear_models/prehlt_and_hlt.csv\", index=False)\n",
    "filled_regr_replNovHlts_df[cols_to_keep + ['expression']].to_csv(\"for_linear_models/with_exprs/prehlt_and_hlt_w_exprs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcreat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
